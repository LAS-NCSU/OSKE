package edu.ncsu.las.source;

import edu.ncsu.las.collector.Collector;
import edu.ncsu.las.model.collector.Configuration;
import edu.ncsu.las.model.collector.CrawlConfigWithJSON;
import edu.ncsu.las.model.collector.SearchRecord;
import edu.ncsu.las.model.collector.type.ConfigurationType;
import edu.ncsu.las.model.collector.type.JobHistoryStatus;
import edu.ncsu.las.model.collector.type.SourceParameter;
import edu.ncsu.las.model.collector.type.SourceParameterType;
import edu.ncsu.las.source.util.ProcessConfiguration;
import edu.ncsu.las.source.util.SourceCrawler;
import edu.ncsu.las.util.FileUtilities;
import edu.ncsu.las.util.InternetUtilities;
import edu.uci.ics.crawler4j.crawler.CrawlController;
import edu.uci.ics.crawler4j.crawler.RobotsDenyNotice;
import edu.uci.ics.crawler4j.fetcher.PageFetcher;
import edu.uci.ics.crawler4j.robotstxt.RobotstxtConfig;
import edu.uci.ics.crawler4j.robotstxt.RobotstxtServer;
import edu.uci.ics.crawler4j.url.WebURL;

import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Path;
import java.util.List;
import java.util.logging.Level;

import org.json.JSONObject;



/**
 * This is the primary class used to surf the internet
 * 
 * Start seeds may have FormatTags within them and be expanded...
 * see InternetUtilities.expandURLs
 * 
 */
public class WebSourceHandler extends AbstractHandler implements RobotsDenyNotice, DomainDiscoveryInterface {

	// TODO make a note in the configuration guide that the exclude filters are additive...

	private static final JSONObject SOURCE_HANDLER_CONFIGURATION = new JSONObject("{  'excludeFilter' : '.*(\\\\.(css|js))$', \"webCrawler\":{\"maxPagesToFetch\": 10000,\"politenessDelay\":200,\"includeBinaryContentInCrawling\":true,\"maxDepthOfCrawling\":2,\"respectRobotsTxt\":true},\"limitToHost\":true}");
	private static final String SOURCE_HANDLER_NAME = "web";
	private static final String SOURCE_HANDLER_DISPLAY_NAME = "Web Crawler";
	
	
	private static final java.util.TreeMap<String, SourceParameter> SOURCE_HANDLER_PARAM_CONFIG = new java.util.TreeMap<String, SourceParameter>() {
		private static final long serialVersionUID = 1L;
		{
			put("excludeFilter", new SourceParameter("excludeFilter","regular expression that if matches the URL should not be crawled. (blacklist)", false,"",false,SourceParameterType.REGEX,false,false));
			put("excludeWikiSpecialPages", new SourceParameter("excludeWikiSpecialPages","If the current page was generated by MediaWiki, should URLs that link to special special pages and actions be skipped?", false,"",false,SourceParameterType.BOOLEAN,false,false));
			put("includeFilter", new SourceParameter("includeFilter", "regular expression that a URL must match to be crawled. (whitelist)", false,"",false,SourceParameterType.REGEX,false,false));
		    put("limitToDomain", new SourceParameter("limitToDomain", "true/false - should the crawler stay on domain (hosts) specified in the seed", false,"true",false,SourceParameterType.BOOLEAN,false,true));
		    put("limitToHost",   new SourceParameter("limitToHost", "true/false - should the crawler stay on the same host as specified in the seed", false,"true",false,SourceParameterType.BOOLEAN,false,false));
		    put("limitToLanguage", new SourceParameter("limitToLanguage", "array of language two character abbreviations (see ISO 639-1) that text must be one of. If another language, then the page is considered not relevant.", false,"[\"en\"]",true,SourceParameterType.STRING,false,false)); 
		    put("allowSingleHopFromReferrer",   new SourceParameter("allowSingleHopFromReferrer", "true/false - should the crawler allow a hop to another domain if the referring page was valid.", false,"false",false,SourceParameterType.BOOLEAN,false,false));
		    put("allowDomains",   new SourceParameter("allowDomains", "array of domains names that may be crawled.", false,"[\"archive.org\",\"wikidata.org\"]",true,SourceParameterType.STRING,false,false));
		    put("startsWithPath",new SourceParameter("startsWithPath", "if set, content from the server must start with this path", false,"",false,SourceParameterType.STRING,false,false));
		    put("relevantRegExp",new SourceParameter("relevantRegExp", "if set, uses a case-insensitive match to see if the page contains the expression to determine relevancy", false,"",false,SourceParameterType.REGEX,false,false));
		    put("ignoreRelevancyForInitialURL",new SourceParameter("ignoreRelevancyForInitialURL", "true/false - if true, will not preform a relevancy check on starting (initial/seed) URLs when looking at other links to crawl", false,"",false,SourceParameterType.BOOLEAN,false,false));
		    put("ignoreRelevancyForImages",new SourceParameter("ignoreRelevancyForImages", "true/false - if true, will not preform a relevancy check on files whose content type contains \"image\".", false,"",false,SourceParameterType.BOOLEAN,false,false));
		    put("textExtractionMethod",new SourceParameter("textExtractionMethod", "method used to extract text from HTML pages. Possible values: \"tika\", \"jsoup\",\"boilerpipe\".  Default \"boilerpipe\".  If fails, \"tika\" is used.", false,"",false,SourceParameterType.STRING,false,false));
		    put("downloadVideo", new SourceParameter("downloadVideo", "true/false - should the crawler download videos.  Defaults to false if not set", false,"",false,SourceParameterType.BOOLEAN,false,false));
		    put("process",   new SourceParameter("process","JavaScript object consisting of 1 or more of the following fields.  if any of the exclude options are meet, then the page is not processed.  Only one of the include conditions need to be met if present", false,"",false,SourceParameterType.JSON_OBJECT,false,false));
		    put("process.includeTextRegExp", new SourceParameter("process.includeTextRegExp","text on a page must match this regular expression to be stored and processed", false,"",false,SourceParameterType.REGEX,false,false));
		    put("process.includeURLRegExp",  new SourceParameter("process.includeURLRegExp", "The page URL must match this regular expression to be stored and processed", false,"",false,SourceParameterType.REGEX,false,false));
		    put("process.includeMimeType",   new SourceParameter("process.includeMimeType",  "The page must have one of these mime types (comma separated) to be storeed and processed", false,"application/pdf",false,SourceParameterType.STRING,false,false));
		    put("process.excludeTextRegExp", new SourceParameter("process.excludeTextRegExp","if text on a page matches this regular expression, it will not be stored/processed", false,"",false,SourceParameterType.REGEX,false,false));
		    put("process.excludeURLRegExp",  new SourceParameter("process.excludeURLRegExp", "if the url for a page matches this regular expression, the page will not be stored/processed", false,"",false,SourceParameterType.REGEX,false,false));
		    put("process.excludeMimeType",   new SourceParameter("process.excludeMimeType",  "comma-separated list of mime types.  If the page mime type matches, it will not be stored/processed", false,"",false,SourceParameterType.STRING,false,false));		    
		    put("webCrawler",    new SourceParameter("webCrawler","JavaScript object consisting of 1 or more of the following fields", false,"",false,SourceParameterType.JSON_OBJECT,false,true));
		    put("webCrawler.politenessDelay",    new SourceParameter("webCrawler.politenessDelay","how many milliseconds should be the delay between page requesets", false,"200",false,SourceParameterType.INT,false,true));
		    put("webCrawler.maxDepthOfCrawling", new SourceParameter("webCrawler.maxDepthOfCrawling","how many links should be followed. -1 for unlimited.  0 for just the seed", false,"2",false,SourceParameterType.INT,false,true));
		    put("webCrawler.maxPagesToFetch",    new SourceParameter("webCrawler.maxPagesToFetch","what are the maximum number of pages to return. -1 for unlimited.", false,"2000",false,SourceParameterType.INT,false,true));	
		    put("webCrawler.maxDownloadSize",    new SourceParameter("webCrawler.maxDownloadSize", "Maximum size of a page to return in bytes.", false,"20000000",false,SourceParameterType.INT,false,true));
		    put("webCrawler.numCrawlingThreads" ,new SourceParameter("webCrawler.numCrawlingThreads", "How many threads should be used in the crawling process. Defaults to 1.", false,"1",false,SourceParameterType.INT,false,false));
		    put("webCrawler.respectRobotsTxt",    new SourceParameter("webCrawler.respectRobotsTxt", "true/false.  Should the job respect a site's robots.txt file.  Default is true.",false,"true",false,SourceParameterType.BOOLEAN,false,true));
		    put("webCrawler.userAgentString",    new SourceParameter("webCrawler.userAgentString", "What UserAgent string should be sent by the crawler.",false,"",false,SourceParameterType.STRING,false,false));
		    put("webCrawler.includeBinaryContentInCrawling", new SourceParameter("webCrawler.includeBinaryContentInCrawling", "true/false - should the crawler return binary content", false,"",false,SourceParameterType.BOOLEAN,false,true));
		    put("webCrawler.headers", new SourceParameter("webCrawler.headers", "JSON Array of Strings, each line in the format of Header-Name:value", false,"",true,SourceParameterType.STRING,false,false));
		    put("webCrawler.authenticationBasic", new SourceParameter("webCrawler.authenticationBasic","JavaScript object consisting of the following 3 fields to use basic authentication when crawling a site.  Logout pages should be specified in the exclude filter", false,"",false,SourceParameterType.JSON_OBJECT,false,false));
		    put("webCrawler.authenticationBasic.username", new SourceParameter("webCrawler.authenticationBasic.username","what is the user's identity to access the site?", false,"",false,SourceParameterType.STRING,false,false));
		    put("webCrawler.authenticationBasic.password", new SourceParameter("webCrawler.authenticationBasic.password","what is the user's password?  This should be entered as cleartext.  The value will be encrypted (using AES) and stored with a {AES} prefix.", false,"",false,SourceParameterType.STRING,true,false));
		    put("webCrawler.authenticationBasic.loginURL", new SourceParameter("webCrawler.authenticationBasic.loginURL","what is the URL used to authenticate the user?", false,"",false,SourceParameterType.STRING,false,false));
		    put("webCrawler.authenticationForm", new SourceParameter("webCrawler.authenticationForm","JavaScript object consisting of the following 5 fields to use Form authentication when crawling a site.  Logout pages should be specified in the exclude filter", false,"",false,SourceParameterType.JSON_OBJECT,false,false));
		    put("webCrawler.authenticationForm.username", new SourceParameter("webCrawler.authenticationForm.username","what is the user's identity to access the site?", false,"",false,SourceParameterType.STRING,false,false));
		    put("webCrawler.authenticationForm.password", new SourceParameter("webCrawler.authenticationForm.password","what is the user's password?  This should be entered as cleartext.  The value will be encrypted (using AES) and stored with a {AES} prefix.  If you start with {hashMD5}, then the following value will be hashed with MD5, and then encrypted.", false,"",false,SourceParameterType.STRING,true,false));
		    put("webCrawler.authenticationForm.loginURL", new SourceParameter("webCrawler.authenticationForm.loginURL","what is the URL used to authenticate the user?", false,"",false,SourceParameterType.STRING,false,false));
		    put("webCrawler.authenticationForm.userFieldName", new SourceParameter("webCrawler.authenticationForm.userFieldName","what is the field name used to specify the user's identity?", false,"",false,SourceParameterType.STRING,false,false));
		    put("webCrawler.authenticationForm.passwordFieldName", new SourceParameter("webCrawler.authenticationForm.passwordFieldName","what is the field name used to specify/enter user's password? ", false,"",false,SourceParameterType.STRING,false,false));
		    put("webCrawler.authenticationForm.additionalFormData", new SourceParameter("webCrawler.authenticationForm.additionalFormData","Array of JSON objects.  Each object has fieldName and fieldValue.  Used to set additional properties during the login process.", false,"",true,SourceParameterType.JSON_OBJECT,false,false));
		    put("webCrawler.authenticationForm.additionalFormData.fieldName", new SourceParameter("webCrawler.authenticationForm.additionalFormData.fieldName","name of a field to pass to the server during a form-based authentication.", false,"",false,SourceParameterType.STRING,false,false));
		    put("webCrawler.authenticationForm.additionalFormData.fieldValue", new SourceParameter("webCrawler.authenticationForm.additionalFormData.fieldValue","value of a field to pass to the server during a form-based authentication.", false,"",false,SourceParameterType.STRING,false,false));
		    
		    put("webCrawler.authenticationNTLM", new SourceParameter("webCrawler.authenticationNTLM","JavaScript object consisting of the following 3 fields to use NTLM authentication when crawling a site.  Logout pages should be specified in the exclude filter", false,"",false,SourceParameterType.JSON_OBJECT,false,false));
		    put("webCrawler.authenticationNTLM.username", new SourceParameter("webCrawler.authenticationNTLM.username","what is the user's identity to access the site?", false,"",false,SourceParameterType.STRING,false,false));
		    put("webCrawler.authenticationNTLM.password", new SourceParameter("webCrawler.authenticationNTLM.password","what is the user's password?  This should be entered as cleartext.  The value will be encrypted (using AES) and stored with a {AES} prefix.", false,"",false,SourceParameterType.STRING,true,false));
		    put("webCrawler.authenticationNTLM.domain", new SourceParameter("webCrawler.authenticationNTLM.domain","what is the user's NT domain?", false,"",false,SourceParameterType.STRING,false,false));
		    put("webCrawler.authenticationNTLM.loginURL", new SourceParameter("webCrawler.authenticationNTLM.loginURL","what is the URL used to authenticate the user? probably not needed, enter the seed value", false,"",false,SourceParameterType.STRING,false,false));
		    
		    put("extractArea",       new SourceParameter("extractArea","Contains an array of JSON objects with title & selector attributes to be used as the data to extract from an HTML page", false,"",true, SourceParameterType.JSON_OBJECT,false,false));
		    put("extractArea.title", new SourceParameter("extractArea.title", "Used as a section title when outputing plain text from HTML. Can be left blank", false,"",false, SourceParameterType.STRING,false,false));
		    put("extractArea.selector", new SourceParameter("extractArea.selector", "CSS selector used to identify portion(s) of an HTML page to extract.  Syntax available at  http://jsoup.org/cookbook/extracting-data/selector-syntax", false,"",false, SourceParameterType.STRING,false,false));
	}};
	


	@Override
	public boolean supportsDomainDiscovery() {
		return true;
	}
	
	
	private ProcessConfiguration _processCheckConfiguration = null;  // tracks the process parameters.  ie, the parameters that guide whether or not to process a page
	
	public ProcessConfiguration getProcessCheckConfiguration() {
		if (_processCheckConfiguration == null) {
			_processCheckConfiguration = new ProcessConfiguration(this, this.getJob().getConfiguration());
		}
		return _processCheckConfiguration;
	}
	
		

	public void process() {
		List<String> seeds = InternetUtilities.expandURLs(this.getJob().getPrimaryFieldValue());
		
		this.processInternal(seeds);
	}
	
	private CrawlController controller = null;
	
	public void processInternal(java.util.Collection<String> seeds) {
		srcLogger.log(Level.INFO, "Job procesing starting: " + this.getJob().getPrimaryFieldValue());
		
		// Finish configuring the document with the relevancy pattern
		this.getDocumentRouter().setRelevancyPattern(this.getProcessCheckConfiguration().getRelevancyPattern());
		

		// Establish temporary working directory
		Path tempDirectory;
		try {
			tempDirectory = Files.createTempDirectory(Configuration.getConfigurationProperty(this.getDomainInstanceName(),edu.ncsu.las.model.collector.type.ConfigurationType.COLLECTOR_WORKINGDIRECTORY));
		} catch (IOException ioex) {
			String message = "Unable to create working directory at "+ Configuration.getConfigurationProperty(this.getDomainInstanceName(),edu.ncsu.las.model.collector.type.ConfigurationType.COLLECTOR_WORKINGDIRECTORY);
			srcLogger.log(Level.SEVERE, message);
			srcLogger.log(Level.SEVERE, "IOException: " + ioex);
			this.getJobCollector().sourceHandlerCompleted(this, JobHistoryStatus.ERRORED, message);
			this.setJobHistoryStatus(JobHistoryStatus.ERRORED);
			return;
		}
		srcLogger.log(Level.INFO, "Working directory established: " + tempDirectory);

		// create configuration. job > sourHandler > systemConfig
		CrawlConfigWithJSON crawlConfig = new CrawlConfigWithJSON();
		crawlConfig.setFollowRedirects(true);
		crawlConfig.setCrawlStorageFolder(tempDirectory.toString());
		crawlConfig.performConfiguration(this.getDomainInstanceName(),Configuration.getConfigurationObject(this.getDomainInstanceName(),edu.ncsu.las.model.collector.type.ConfigurationType.WEBCRAWLER));
		if (this.getHandlerDefaultConfiguration().has("webCrawler")) {
			crawlConfig.performConfiguration(this.getDomainInstanceName(),this.getHandlerDefaultConfiguration().getJSONObject("webCrawler"));
		}
		if (this.getJob().getConfiguration().has("webCrawler")  ) {
			JSONObject webCrawlerObject = this.getJob().getConfiguration().optJSONObject("webCrawler");
			if (webCrawlerObject == null) { webCrawlerObject = new JSONObject(); }
			crawlConfig.performConfiguration(this.getDomainInstanceName(),webCrawlerObject);
		}

		PageFetcher pageFetcher = new PageFetcher(crawlConfig);
		RobotstxtConfig robotstxtConfig = new RobotstxtConfig();
		robotstxtConfig.setEnabled(crawlConfig.getRespectRobotsTxtFile());
		robotstxtConfig.setUserAgentName(SourceHandlerInterface.getNextUserAgent(this.getDomainInstanceName()));
		//robotstxtConfig.setUserAgentName(crawlConfig.getUserAgentString());
		RobotstxtServer robotstxtServer = new RobotstxtServer(robotstxtConfig, pageFetcher);

		try {
			controller = new CrawlController(crawlConfig, pageFetcher, robotstxtServer);
			controller.setCustomData(this);
			
			for (String s: seeds) {
				if (s == null || s.toLowerCase().startsWith("http")==false) {
					srcLogger.log(Level.SEVERE, "Invalid URL specified as seed - Job: "+ this.getJob().getName() +", seed url: " + s);
				}
				else {
					java.net.URL test = new java.net.URL(s);
					if (Collector.getTheCollecter().getDomain(this.getDomainInstanceName()).isTopLevelDomainBlocked(test.getHost())) {
						srcLogger.log(Level.INFO, "Top level domain blocked -  seed url: " + s);	
						this.addBlockedURL("blocked TLD:", s);
					}
					else {
						controller.addSeed(s);
					}
				}
			}

			srcLogger.log(Level.INFO, "start crawling: " + this.getJob().getPrimaryFieldValue());
			controller.start(SourceCrawler.class, crawlConfig.getNumCrawlingThreads());  // TODO:  this is no longer true ....  WARNING!!!  THIS CREATES A NEW INSTANCE OF WebSourceHandler.  Need to use this incase we have children who have overridden our methods
			
			// TODO -> need to determine if the controller stopped on its own or was stopped.
			if (this.isManuallyStopped()) {
				srcLogger.log(Level.INFO, "crawling stopped: " +  this.getJob().getPrimaryFieldValue());
			}
			else {
				srcLogger.log(Level.INFO, "crawling complete: " +  this.getJob().getPrimaryFieldValue());
			}

		} catch (Exception e1) {
			srcLogger.log(Level.SEVERE, "Unable to create crawling controller: " + e1);
			this.getJobCollector().sourceHandlerCompleted(this, JobHistoryStatus.ERRORED, "Unable to create crawling controller: " + e1);
			this.setJobHistoryStatus(JobHistoryStatus.ERRORED);
			return;
		}

		// clean up temporary directory
		try {
			FileUtilities.deleteDirectory(tempDirectory);
		} catch (IOException e) {
			srcLogger.log(Level.SEVERE, "Unable to delete working directory at " + tempDirectory);
			srcLogger.log(Level.SEVERE, "IOException: " + e);
		}
		srcLogger.log(Level.INFO, "Job procesing complete: " + this.getJob().getPrimaryFieldValue());
		JobHistoryStatus status = JobHistoryStatus.COMPLETE;
		String message = "";
		if (this.isManuallyStopped()) {
			status=JobHistoryStatus.STOPPED;
			message = "Job stopped upon request.";
		}
		
		if (this.getDeniedURLByRobotsTxtCount() >0 ) {
			message += this.getDeniedURLByRobotsTxtCount() + " url(s) skipped due to robots.txt.";
		}
		if (this.getDeniedSeedURLs().size()>0) {
			
			message += "Skipped seed URLs from robots.txt: " + String.join(",", this.getDeniedSeedURLs());
			
		}
		
		this.getJobCollector().sourceHandlerCompleted(this, status,message);
		this.setJobHistoryStatus(status);
		return;
	}

	@Override
	public JSONObject getHandlerDefaultConfiguration() {
		return SOURCE_HANDLER_CONFIGURATION;
	}

	@Override
	public String getSourceHandlerName() {
		return SOURCE_HANDLER_NAME;
	}

	@Override
	public String getSourceHandlerDisplayName() {
		return SOURCE_HANDLER_DISPLAY_NAME;
	}		
	
	@Override
	public String getDescription() {
		return "The web source crawler is the primary crawler for crawling web sites.";
	}
	
	@Override
	public ParameterLabelType getPrimaryLabel() {
		return ParameterLabelType.URL;
	}
	

	@Override
	public java.util.TreeMap<String, SourceParameter> getConfigurationParameters() {
		return SOURCE_HANDLER_PARAM_CONFIG;
	}
		
	
	@Override
	public boolean stop() {
		super.stop();
		
		controller.shutdown();
		
		return true;
	}

	@Override
	public void forceShutdown() {
		// Need to get the actual instance of the controller as another object was created when the crawler started.
		controller.getPageFetcher().shutDown();
		controller.shutdown();
	}
	
	private SourceParameterRepository _sourceParameters;

	@Override
	public SourceParameterRepository getSourceParameterRepository() {
		if (_sourceParameters == null) {
			_sourceParameters = new SourceParameterRepository(this.getConfigurationParameters());
		}

		return _sourceParameters;
	}		
	
	// track seed URLs that were denied
	private java.util.Vector<String> deniedSeedURLs = new java.util.Vector<String>();
	private java.util.concurrent.atomic.AtomicInteger deniedURLCounter = new java.util.concurrent.atomic.AtomicInteger(0);

	
	
	
	public void urlDeniedByRobotsTxt(WebURL webURL, boolean asSeeed) {
		if (asSeeed) {
			deniedSeedURLs.add(webURL.getURL());
			this.addBlockedURL("seedRobotsText", webURL.getURL());
			srcLogger.log(Level.WARNING, "Seed URL blocked by robots.txt.  Job Name: "+this.getJob().getName()+", URL: "+webURL.getURL());
		}
		else {
			deniedURLCounter.incrementAndGet();
			this.addBlockedURL("pageRobotsText", webURL.getURL());
			srcLogger.log(Level.INFO, "URL blocked by robots.txt.  Job Name: "+this.getJob().getName()+", URL: "+webURL.getURL());
		}		
	}
	
	/**
	 * Returns the number of URLs denied by robots.txt 
	 * 
	 * @return integer value matching the number of blocked URLs
	 */
	public int getDeniedURLByRobotsTxtCount() {
		return deniedURLCounter.get();
	}
	
	public java.util.Vector<String> getDeniedSeedURLs() {
		return deniedSeedURLs;
	}
	

	public void addURLToCrawl(String url, int depth) {
		
		SourceCrawler sc = new SourceCrawler(this);
				
		WebURL wu = new WebURL();
		wu.setURL(url);
		if (sc.shouldVisit(null, wu)) {
			srcLogger.log(Level.INFO, "Added URL back to a source handler: "+url);
			controller.addSeed(url,-1,depth+1);
		}
		else {
			srcLogger.log(Level.INFO, "Not adding url as shouldVisit criteria was not met to a source handler: "+url);
		}
	}


	/**
	 * simply produces a search records based upon the passed into URL to be used within the domain discovery area.
	 */
	@Override
	public List<SearchRecord> generateSearchResults(String domain, String urlOrSearchTerms, JSONObject configuration, int numResults, JSONObject advConfiguration) {
		List<SearchRecord> records = new java.util.ArrayList<SearchRecord>(); 
		records.add(new SearchRecord(urlOrSearchTerms,urlOrSearchTerms,"",1,"websource"));
		return records;
	}

	/**
	 * For the most part, we will check the the parameters through access the superclass.  The websource handler will check whether or
	 * not we can use authenticated access.
	 *
	 * @return list of possible issues. Array will be empty if no issues found.
	 */
	@Override
	public List<String> validateConfiguration(String domain, String primaryField, JSONObject configuration) {
		java.util.List<String> errors = super.validateConfiguration(domain, primaryField, configuration);
				
		boolean allowAuthentication = Configuration.getConfigurationPropertyAsBoolean(domain, ConfigurationType.ACCESS_AUTHENTICATED_SITES);
		if (!allowAuthentication && configuration.has("webCrawler")) {
			JSONObject webCrawlerObject = configuration.getJSONObject("webCrawler");
			if (webCrawlerObject.has("authenticationBasic") || webCrawlerObject.has("authenticationForm") || webCrawlerObject.has("authenticationNTLM")) {
				errors.add("Domain configuration does not allow access to authenticated sites");
			}
		}
		
		return errors;
	}	
	
}
